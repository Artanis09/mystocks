# -*- coding: utf-8 -*-
"""
Inference - ì‹¤ì‹œê°„ ì˜ˆì¸¡ ë° ì¶”ì²œ ì¢…ëª© ìƒì„±
"""
import argparse
import sys
from pathlib import Path

# Allow running as a script: `python ml/inference.py`
_ROOT = Path(__file__).resolve().parents[1]
if str(_ROOT) not in sys.path:
    sys.path.insert(0, str(_ROOT))

import os
import glob
import json
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional, Literal, Any

import numpy as np
import pandas as pd
from catboost import CatBoostClassifier
import re

from ml.config import CFG
from ml.data_pipeline import load_all_bars, compute_technical_indicators, get_feature_columns


def get_suspended_codes_recent(lookback_days: int = 3) -> set:
    """
    ìµœê·¼ N ì˜ì—…ì¼ ë‚´ ê±°ë˜ì •ì§€ ì´ë ¥(volume=0)ì´ ìˆëŠ” ì¢…ëª© ì½”ë“œ ì§‘í•© ë°˜í™˜.
    - bars íŒŒí‹°ì…˜ì—ì„œ ìµœê·¼ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì—¬ volume=0ì¸ ì¢…ëª©ì„ ì°¾ìŒ
    """
    bars_dir = CFG.bars_dir
    suspended_codes = set()
    
    # ìµœê·¼ N ì˜ì—…ì¼ ì°¾ê¸° (ì£¼ë§ ì œì™¸)
    recent_dates = []
    check_date = datetime.now()
    while len(recent_dates) < lookback_days:
        check_date -= timedelta(days=1)
        if check_date.weekday() < 5:  # í‰ì¼ë§Œ
            recent_dates.append(check_date.strftime("%Y-%m-%d"))
    
    for date_str in recent_dates:
        partition_path = os.path.join(bars_dir, f"date={date_str}", "part-0000.parquet")
        if os.path.exists(partition_path):
            try:
                df = pd.read_parquet(partition_path)
                if "volume" in df.columns and "code" in df.columns:
                    # volumeì´ 0 ë˜ëŠ” NaNì¸ ì¢…ëª© ì¶”ì¶œ
                    zero_vol = df[(df["volume"].isna()) | (df["volume"] == 0)]
                    suspended_codes.update(zero_vol["code"].apply(lambda x: str(x).zfill(6)).tolist())
            except Exception as e:
                print(f"[WARN] Failed to read {partition_path}: {e}")
    
    return suspended_codes


def resolve_model_path(model_name: str, explicit_path: Optional[str] = None) -> str:
    """ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œë¡œ ì‹¤ì œ ëª¨ë¸ ê²½ë¡œë¥¼ ê²°ì •."""
    if explicit_path:
        return explicit_path

    pattern = 'catboost_*.cbm'
    if model_name == 'model5':
        pattern = 'lgbm_model5_*.txt'

    model_files = glob.glob(os.path.join(CFG.model_dir, pattern))
    if not model_files:
        raise FileNotFoundError(f"No trained model found for pattern {pattern}. Run training first.")
    return max(model_files, key=os.path.getctime)


def _meta_path_for_model(model_path: str) -> str:
    if model_path.endswith('.cbm'):
        return model_path.replace('.cbm', '_meta.json')
    if model_path.endswith('.txt'):
        return model_path.replace('.txt', '_meta.json')
    return model_path + '_meta.json'


def _infer_positive_return_from_meta(meta: Optional[dict], default: float) -> float:
    """Infer a representative positive return for expected_return scoring.

    This value is only used for ranking; any positive scalar keeps the same order.
    """
    if not meta:
        return default

    definition = str(meta.get('definition') or '')
    # e.g. "... >= 0.02"
    m = re.search(r">=\s*(0\.\d+)", definition)
    if m:
        try:
            return float(m.group(1))
        except ValueError:
            return default
    return default


def load_model(model_path: str = None, model_name: str = 'model1') -> Tuple[Any, dict]:
    """
    í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
    """
    if model_path is None:
        model_path = resolve_model_path(model_name, None)

    if model_name == 'model5' or str(model_path).endswith('.txt'):
        try:
            import lightgbm as lgb
        except ModuleNotFoundError as e:
            raise ModuleNotFoundError(
                "model5 requires 'lightgbm'. Install with: pip install lightgbm (or pip install -r requirements.txt)"
            ) from e

        model = lgb.Booster(model_file=model_path)
    else:
        model = CatBoostClassifier()
        model.load_model(model_path)
    
    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    meta_path = _meta_path_for_model(model_path)
    meta = {}
    if os.path.exists(meta_path):
        with open(meta_path, 'r', encoding='utf-8') as f:
            meta = json.load(f)
    
    print(f"[INFO] Model loaded: {model_path}")
    return model, meta


def get_recent_data(lookback_days: int = 300) -> pd.DataFrame:
    """
    ìµœê·¼ Nì¼ê°„ì˜ ë°ì´í„° ë¡œë“œ (í”¼ì²˜ ê³„ì‚°ì„ ìœ„í•œ ì¶©ë¶„í•œ ê¸°ê°„)
    """
    bars_dir = CFG.bars_dir
    partitions = sorted([
        d for d in os.listdir(bars_dir) 
        if d.startswith('date=')
    ], reverse=True)
    
    # ìµœê·¼ Nê°œ íŒŒí‹°ì…˜ë§Œ ë¡œë“œ
    recent_partitions = partitions[:lookback_days]
    
    all_data = []
    for partition in recent_partitions:
        path = os.path.join(bars_dir, partition, 'part-0000.parquet')
        if os.path.exists(path):
            df = pd.read_parquet(path)
            # Some daily snapshots may be malformed (e.g., missing 'code').
            # Skip them so they don't poison latest_date detection.
            if 'code' not in df.columns:
                print(f"[WARN] Missing 'code' column in {path}; skipping this partition")
                continue
            all_data.append(df)
    
    if not all_data:
        raise ValueError("No recent data found")
    
    result = pd.concat(all_data, ignore_index=True)
    result['date'] = pd.to_datetime(result['date'])
    result = result.sort_values(['code', 'date']).reset_index(drop=True)
    
    return result


def prepare_inference_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    ì¶”ë¡ ì„ ìœ„í•œ í”¼ì²˜ ì¤€ë¹„ (ìµœì‹  ë‚ ì§œë§Œ)
    """
    # ì¢…ëª©ë³„ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
    results = []
    for code, group in df.groupby('code'):
        group = group.sort_values('date').reset_index(drop=True)
        if len(group) < 60:
            continue
        
        processed = compute_technical_indicators(group)
        # ë§ˆì§€ë§‰ í–‰ë§Œ (ê°€ì¥ ìµœê·¼ ë‚ ì§œ)
        # 0.8 í™•ë¥  ì „ëµì— í•„ìš”í•œ open ê°€ê²© ìœ ì§€ë¥¼ ìœ„í•´ select ì‹œ í¬í•¨ í™•ì¸
        results.append(processed.iloc[-1:])
    
    if not results:
        raise ValueError("No valid data for inference")
    
    return pd.concat(results, ignore_index=True)


def predict_next_day(
    model: Any,
    df: pd.DataFrame,
    top_k: int = 5,
    min_prob_threshold: float = 0.70,
    min_market_cap_krw: float = 50_000_000_000,
    daily_strength_min: float = -0.05,
    return_1d_min: Optional[float] = -0.05,
    upper_lock_cut: Optional[float] = 0.295,
    model_name: str = 'model1',
    meta: Optional[dict] = None,
    filter_suspended_days: int = 3,  # ìµœê·¼ Nì˜ì—…ì¼ ê±°ë˜ì •ì§€ ì´ë ¥ í•„í„°
) -> pd.DataFrame:
    """
    ë‹¤ìŒ ë‚  ìƒìŠ¹ ì˜ˆì¸¡ - ìµœì¢… í•„í„° ì¡°ê±´ ì ìš©
    1. í™•ë¥  >= min_prob_threshold
    2. ì‹œê°€ì´ì•¡ >= min_market_cap_krw (ê¸°ë³¸ 500ì–µ)
    3. ìµœê·¼ Nì˜ì—…ì¼ ì´ë‚´ ê±°ë˜ì •ì§€ ì´ë ¥(volume=0) ì—†ìŒ
    3. ë‹¹ì¼ ì‹œê°€ ëŒ€ë¹„ ì¢…ê°€ ë³€ë™ë¥  >= daily_strength_min (ê¸°ë³¸ -5%)
    4. ì „ì¼ëŒ€ë¹„ ìˆ˜ìµë¥  return_1d >= return_1d_min (ì˜µì…˜)
    5. ìƒí•œê°€(ì²´ê²° ë¶ˆê°€) ê·¼ì‚¬ ì œê±°: return_1d < upper_lock_cut (ì˜µì…˜)
    """
    feature_cols = list(meta.get('features')) if (meta and meta.get('features')) else get_feature_columns()
    
    # 1. ì‹œê°€ì´ì•¡ í•„í„° ì¤€ë¹„ (ìƒì¥ì£¼ì‹ìˆ˜ ë¡œë“œ)
    try:
        stocks_info = pd.read_csv('public/korea_stocks.csv')
        stocks_info['ë‹¨ì¶•ì½”ë“œ'] = stocks_info['ë‹¨ì¶•ì½”ë“œ'].apply(lambda x: str(x).zfill(6))
        code_to_shares = dict(zip(stocks_info['ë‹¨ì¶•ì½”ë“œ'], stocks_info['ìƒì¥ì£¼ì‹ìˆ˜']))
        
        df = df.copy()
        df['shares'] = df['code'].map(code_to_shares).fillna(0)
        df['market_cap'] = df['close'] * df['shares']
    except Exception as e:
        print(f"[WARN] Failed to load market cap info: {e}")
        df['market_cap'] = 0  # ì •ë³´ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ í•„í„°ë§ë˜ê²Œ í•¨
    
    # 2. í”¼ì²˜ ì¶”ì¶œ ë° ì˜ˆì¸¡
    X = df[feature_cols]

    # LightGBM model5 (binary)
    if model_name == 'model5' or model.__class__.__name__.lower().startswith('booster'):
        positive_proba = np.asarray(model.predict(X.values))
        if positive_proba.ndim != 1:
            positive_proba = positive_proba.reshape(-1)
        pos_ret = _infer_positive_return_from_meta(meta, default=0.02)
        expected_return = positive_proba * pos_ret
    else:
        proba = model.predict_proba(X.values)
        
        # í™•ë¥ /ê¸°ëŒ€ìˆ˜ìµ ê³„ì‚° (ë©€í‹°í´ë˜ìŠ¤ vs ë°”ì´ë„ˆë¦¬)
        if proba.shape[1] == 2:
            positive_proba = proba[:, 1]
            class_returns = np.array([0.0, 0.035])
        else:
            positive_proba = proba[:, CFG.min_positive_class:].sum(axis=1)
            class_returns = np.array([0.0, 0.035, 0.065, 0.10, 0.155, 0.24, 0.35])

        expected_return = (proba * class_returns).sum(axis=1)
    
    # ê²°ê³¼ DataFrame ìƒì„±
    base_cols = ['date', 'code', 'close', 'open', 'volume', 'market_cap']
    if 'return_1d' in df.columns:
        base_cols.append('return_1d')
    result = df[base_cols].copy()
    result['positive_proba'] = positive_proba
    result['expected_return'] = expected_return

    # ë‹¹ì¼ ë“±ë½ë¥ (ì‹œê°€â†’ì¢…ê°€)
    result['intraday_return'] = (result['close'] - result['open']) / result['open']
    
    # 3. ìµœì¢… í•„í„° ì ìš©
    # 3.1 í™•ë¥  ì„ê³„ê°’
    mask = (result['positive_proba'] >= min_prob_threshold)
    # 3.2 ì‹œê°€ì´ì•¡
    mask &= (result['market_cap'] >= min_market_cap_krw)
    # 3.3 ê±°ë˜ì •ì§€ ì œì™¸ (ë‹¹ì¼ ê±°ë˜ëŸ‰ 0 í•„ìˆ˜ í•„í„°)
    mask &= (result['volume'] > 0)
    
    # 3.3.1 ìµœê·¼ Nì˜ì—…ì¼ ì´ë‚´ ê±°ë˜ì •ì§€ ì´ë ¥ ì œì™¸
    if filter_suspended_days and filter_suspended_days > 0:
        suspended_codes = get_suspended_codes_recent(lookback_days=filter_suspended_days)
        if suspended_codes:
            result['code_padded'] = result['code'].apply(lambda x: str(x).zfill(6))
            mask &= ~result['code_padded'].isin(suspended_codes)
            print(f"[INFO] Filtering out {len(suspended_codes)} stocks with volume=0 in last {filter_suspended_days} days")
    
    # 3.4 ë‹¹ì¼ ì‹œê°€ ëŒ€ë¹„ ì¢…ê°€ ë³€ë™ë¥  (ì¥ëŒ€ìŒë´‰ ì œì™¸)
    mask &= (result['intraday_return'] >= daily_strength_min)

    # 3.5 ì „ì¼ ëŒ€ë¹„ ìˆ˜ìµë¥  í•„í„° (ë„ˆë¬´ ì„¼ ìŒë´‰ ì œê±°)
    if 'return_1d' in result.columns and return_1d_min is not None:
        mask &= (result['return_1d'] >= return_1d_min)

    # 3.6 ìƒí•œê°€(ì²´ê²° ë¶ˆê°€) ê·¼ì‚¬ ì œê±°
    if 'return_1d' in result.columns and upper_lock_cut is not None:
        mask &= (result['return_1d'] < upper_lock_cut)
    
    candidates = result[mask].copy()
    
    # ê¸°ëŒ€ ìˆ˜ìµë¥  ìˆœìœ¼ë¡œ ì •ë ¬
    candidates = candidates.sort_values('expected_return', ascending=False)
    
    # ìƒìœ„ Kê°œ ì„ íƒ
    top_candidates = candidates.head(top_k)
    
    return top_candidates


def get_stock_name_mapping() -> Dict[str, str]:
    """
    ì¢…ëª©ì½”ë“œ-ì¢…ëª©ëª… ë§¤í•‘ ë¡œë“œ
    """
    # 1. korea_stocks.csv (ìµœì‹  ìƒì¥ì£¼ì‹ìˆ˜ í¬í•¨)
    csv_path = 'public/korea_stocks.csv'
    if os.path.exists(csv_path):
        try:
            df = pd.read_csv(csv_path)
            df['ë‹¨ì¶•ì½”ë“œ'] = df['ë‹¨ì¶•ì½”ë“œ'].apply(lambda x: str(x).zfill(6))
            return dict(zip(df['ë‹¨ì¶•ì½”ë“œ'], df['í•œê¸€ ì¢…ëª©ì•½ëª…']))
        except:
            pass

    # 2. tickers.parquet (ë°±ì—…)
    tickers_path = os.path.join('data/krx/master/tickers.parquet')
    if os.path.exists(tickers_path):
        df = pd.read_parquet(tickers_path)
        return dict(zip(df['code'], df['name']))
    
    return {}


def run_inference(
    model_path: str = None,
    top_k: int = 5,
    model_name: str = 'model1',
    min_prob_threshold: float = 0.70,
    min_market_cap_krw: float = 50_000_000_000,
    daily_strength_min: float = -0.05,
    return_1d_min: Optional[float] = -0.05,
    upper_lock_cut: Optional[float] = 0.295,
    save_result: bool = True
) -> pd.DataFrame:
    """
    ì „ì²´ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    """
    print("=" * 60)
    print("Stock Price Prediction - Inference (Final Strategic Filter)")
    parts = [
        f"Top-{top_k}",
        f"Prob >= {min_prob_threshold*100:.0f}%",
        f"Cap >= {min_market_cap_krw/1e8:,.0f}ì–µ",
        f"Daily >= {daily_strength_min*100:.1f}%",
    ]
    if return_1d_min is not None and upper_lock_cut is not None:
        parts.append(f"return_1d in [{return_1d_min*100:.1f}%, {upper_lock_cut*100:.1f}%)")
    elif return_1d_min is not None:
        parts.append(f"return_1d >= {return_1d_min*100:.1f}%")
    elif upper_lock_cut is not None:
        parts.append(f"return_1d < {upper_lock_cut*100:.1f}%")
    print("Condition: " + ", ".join(parts))
    print("=" * 60)
    
    # ëª¨ë¸ ë¡œë“œ
    model, meta = load_model(model_path, model_name=model_name)
    
    # ìµœê·¼ ë°ì´í„° ë¡œë“œ
    print("\n[INFO] Loading recent data...")
    recent_data = get_recent_data(lookback_days=300)
    latest_date_dt = recent_data['date'].max().normalize()
    latest_date = latest_date_dt.strftime('%Y-%m-%d')
    print(f"[INFO] Latest data date: {latest_date}")
    
    # í”¼ì²˜ ì¤€ë¹„
    print("[INFO] Preparing features...")
    inference_data = prepare_inference_features(recent_data)
    if 'date' in inference_data.columns:
        inference_data = inference_data[inference_data['date'].dt.normalize() == latest_date_dt].copy()
    print(f"[INFO] {len(inference_data)} stocks ready for prediction")
    
    # ì˜ˆì¸¡
    print(f"[INFO] Running Filtered Prediction...")
    predictions = predict_next_day(
        model, 
        inference_data, 
        top_k=top_k,
        min_prob_threshold=min_prob_threshold,
        min_market_cap_krw=min_market_cap_krw,
        daily_strength_min=daily_strength_min,
        return_1d_min=return_1d_min,
        upper_lock_cut=upper_lock_cut,
        model_name=model_name,
        meta=meta,
    )
    
    # ì¢…ëª©ëª… ì¶”ê°€
    name_mapping = get_stock_name_mapping()
    predictions['name'] = predictions['code'].map(name_mapping).fillna('Unknown')
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print(f"ğŸš€ FINAL TOP-{len(predictions)} STRATEGIC PICKS for {latest_date} â†’ Next Day")
    print("=" * 60)
    
    if len(predictions) == 0:
        print("  [WARN] No stocks met the strict filter criteria today.")
    else:
        for i, (idx, row) in enumerate(predictions.iterrows()):
            prob_str = f"{row['positive_proba']*100:.1f}%"
            cap_str = f"{row['market_cap']/1e8:,.0f}ì–µ"
            exp_ret = f"{row['expected_return']*100:.2f}%"
            intra = row.get('intraday_return', np.nan)
            intra_str = "-" if (intra is None or (isinstance(intra, float) and np.isnan(intra))) else f"{intra*100:+.2f}%"
            print(f"  {i+1}. {row['code']} {row['name'][:8]:<8} | "
                  f"ì¢…ê°€: {row['close']:>8,.0f} | "
                  f"ë‹¹ì¼: {intra_str:>7} | "
                  f"ì‹œì´: {cap_str:>7} | "
                  f"ìƒìŠ¹í™•ë¥ : {prob_str:>6} | "
                  f"ê¸°ëŒ€ìˆ˜ìµ: {exp_ret:>6}")
    
    # ê²°ê³¼ ì €ì¥
    if save_result:
        os.makedirs('ml/predictions', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = f'ml/predictions/final_picks_{timestamp}.csv'
        predictions.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"\n[INFO] Strategic picks saved to {output_path}")
    
    return predictions


FilterTag = Literal['filter1', 'filter2']


def run_inference_both(
    model_path: str = None,
    top_k: int = 5,
    save_result: bool = True,
    model_name: str = 'model1',
) -> Dict[FilterTag, pd.DataFrame]:
    """Run both Filter1 and Filter2 on the same latest-day inference snapshot."""
    print("=" * 60)
    print("Stock Price Prediction - Inference (Filter1 vs Filter2)")
    print("=" * 60)

    model, meta = load_model(model_path, model_name=model_name)

    print("\n[INFO] Loading recent data...")
    recent_data = get_recent_data(lookback_days=300)
    latest_date_dt = recent_data['date'].max().normalize()
    latest_date = latest_date_dt.strftime('%Y-%m-%d')
    print(f"[INFO] Latest data date: {latest_date}")

    print("[INFO] Preparing features...")
    inference_data = prepare_inference_features(recent_data)
    if 'date' in inference_data.columns:
        inference_data = inference_data[inference_data['date'].dt.normalize() == latest_date_dt].copy()
    print(f"[INFO] {len(inference_data)} stocks ready for prediction")

    # Filter1 (ê¸°ì¡´ ë¬¸ì„œ ê¸°ì¤€)
    f1 = predict_next_day(
        model,
        inference_data,
        top_k=top_k,
        min_prob_threshold=0.80,
        min_market_cap_krw=50_000_000_000,
        daily_strength_min=-0.05,
        return_1d_min=None,
        upper_lock_cut=None,
        model_name=model_name,
        meta=meta,
    )

    # Filter2 (ìµœì¢… ì ìš©)
    f2 = predict_next_day(
        model,
        inference_data,
        top_k=top_k,
        min_prob_threshold=0.70,
        min_market_cap_krw=50_000_000_000,
        daily_strength_min=-0.05,
        return_1d_min=-0.05,
        upper_lock_cut=0.295,
        model_name=model_name,
        meta=meta,
    )

    name_mapping = get_stock_name_mapping()
    for df_, tag in ((f1, 'filter1'), (f2, 'filter2')):
        if not df_.empty:
            df_['name'] = df_['code'].map(name_mapping).fillna('Unknown')
            df_['filter_tag'] = tag

    def _print_block(title: str, df_: pd.DataFrame):
        print("\n" + "=" * 60)
        print(f"{title} for {latest_date} â†’ Next Day")
        print("=" * 60)
        if df_.empty:
            print("  [WARN] No stocks met the strict filter criteria today.")
            return
        for i, (_, row) in enumerate(df_.iterrows()):
            prob_str = f"{row['positive_proba']*100:.1f}%"
            cap_str = f"{row['market_cap']/1e8:,.0f}ì–µ"
            exp_ret = f"{row['expected_return']*100:.2f}%"
            intra = row.get('intraday_return', np.nan)
            intra_str = "-" if (intra is None or (isinstance(intra, float) and np.isnan(intra))) else f"{intra*100:+.2f}%"
            print(
                f"  {i+1}. {row['code']} {str(row.get('name',''))[:8]:<8} | "
                f"ì¢…ê°€: {row['close']:>8,.0f} | "
                f"ë‹¹ì¼: {intra_str:>7} | "
                f"ì‹œì´: {cap_str:>7} | "
                f"ìƒìŠ¹í™•ë¥ : {prob_str:>6} | "
                f"ê¸°ëŒ€ìˆ˜ìµ: {exp_ret:>6}"
            )

    _print_block("ğŸš€ FILTER1 TOP", f1)
    _print_block("ğŸš€ FILTER2 TOP", f2)

    if save_result:
        os.makedirs('ml/predictions', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        if not f1.empty:
            f1.to_csv(f'ml/predictions/final_picks_filter1_{timestamp}.csv', index=False, encoding='utf-8-sig')
        if not f2.empty:
            f2.to_csv(f'ml/predictions/final_picks_filter2_{timestamp}.csv', index=False, encoding='utf-8-sig')
        both = pd.concat([f1, f2], ignore_index=True)
        both.to_csv(f'ml/predictions/final_picks_both_{timestamp}.csv', index=False, encoding='utf-8-sig')
        print(f"\n[INFO] Strategic picks saved to ml/predictions/final_picks_*_{timestamp}.csv")

    return {'filter1': f1, 'filter2': f2}


def main(argv: Optional[List[str]] = None):
    """CLI entrypoint.

    Examples:
      - python -m ml.inference
      - python -m ml.inference --filter filter2
      - python -m ml.inference --filter both --top-k 10 --no-save
    """
    parser = argparse.ArgumentParser(description="Run ML inference for next-day picks")
    parser.add_argument(
        "--filter",
        choices=["both", "filter1", "filter2"],
        default="both",
        help="which filter to run (default: both)",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=5,
        help="number of picks per filter (default: 5)",
    )
    parser.add_argument(
        "--model-path",
        default=None,
        help="path to a model file (.cbm for CatBoost, .txt for LightGBM) (default: latest in ml/models)",
    )
    parser.add_argument(
        "--model-name",
        choices=["model1", "model5"],
        default="model1",
        help="named model shortcut (model1: 7-class, model5: LightGBM 2%+ binary)",
    )
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="do not write CSV outputs under ml/predictions",
    )

    args = parser.parse_args(argv)

    save_result = not args.no_save
    model_name = args.model_name

    if args.filter == "both":
        results = run_inference_both(
            model_path=args.model_path,
            top_k=args.top_k,
            save_result=save_result,
            model_name=model_name,
        )
    elif args.filter == "filter1":
        results = {
            "filter1": run_inference(
                model_path=args.model_path,
                model_name=model_name,
                top_k=args.top_k,
                min_prob_threshold=0.80,
                min_market_cap_krw=50_000_000_000,
                daily_strength_min=-0.05,
                return_1d_min=None,
                upper_lock_cut=None,
                save_result=save_result,
            )
        }
    else:
        results = {
            "filter2": run_inference(
                model_path=args.model_path,
                model_name=model_name,
                top_k=args.top_k,
                min_prob_threshold=0.70,
                min_market_cap_krw=50_000_000_000,
                daily_strength_min=-0.05,
                return_1d_min=-0.05,
                upper_lock_cut=0.295,
                save_result=save_result,
            )
        }

    print("\n[SUCCESS] Strategic inference completed!")
    return results


if __name__ == "__main__":
    main()
